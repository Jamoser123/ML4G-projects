{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries that are required to run your project\n",
    "# You are allowed to add more libraries as you need\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful to initiate new conda environment given the yml file (Note I adjusted it to work for windows)\n",
    "```bash\n",
    "conda env create -f project1_base.yml -n ml4g_project1\n",
    "```\n",
    "\n",
    "Note: Maybe we rather make a new one, since it hasn't been updated for 4 years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing CAGE-train.zip...\n",
      "  Copied X1_train_info.tsv to CAGE-train/\n",
      "  Copied X1_train_y.tsv to CAGE-train/\n",
      "  Copied X1_val_info.tsv to CAGE-train/\n",
      "  Copied X1_val_y.tsv to CAGE-train/\n",
      "  Copied X2_train_info.tsv to CAGE-train/\n",
      "  Copied X2_train_y.tsv to CAGE-train/\n",
      "  Copied X2_val_info.tsv to CAGE-train/\n",
      "  Copied X1_train_y.tsv to CAGE-train/\n",
      "  Copied X1_val_info.tsv to CAGE-train/\n",
      "  Copied X1_val_y.tsv to CAGE-train/\n",
      "  Copied X2_train_info.tsv to CAGE-train/\n",
      "  Copied X2_train_y.tsv to CAGE-train/\n",
      "  Copied X2_val_info.tsv to CAGE-train/\n",
      "  Copied X2_val_y.tsv to CAGE-train/\n",
      "  Copied X3_test_info.tsv to CAGE-train/\n",
      "  Copied ._X1_train_info.tsv to CAGE-train/\n",
      "  Copied ._X1_train_y.tsv to CAGE-train/\n",
      "  Copied ._X3_test_info.tsv to CAGE-train/\n",
      "  Cleaned up temp_CAGE-train\n",
      "Processing DNase-bed.zip...\n",
      "  Copied X2_val_y.tsv to CAGE-train/\n",
      "  Copied X3_test_info.tsv to CAGE-train/\n",
      "  Copied ._X1_train_info.tsv to CAGE-train/\n",
      "  Copied ._X1_train_y.tsv to CAGE-train/\n",
      "  Copied ._X3_test_info.tsv to CAGE-train/\n",
      "  Cleaned up temp_CAGE-train\n",
      "Processing DNase-bed.zip...\n",
      "  Copied X1.bed -> X1/DNase_X1.bed\n",
      "  Copied X2.bed -> X2/DNase_X2.bed\n",
      "  Copied X3.bed -> X3/DNase_X3.bed\n",
      "  Cleaned up temp_DNase-bed\n",
      "Processing DNase-bigwig.zip...\n",
      "  Copied X1.bed -> X1/DNase_X1.bed\n",
      "  Copied X2.bed -> X2/DNase_X2.bed\n",
      "  Copied X3.bed -> X3/DNase_X3.bed\n",
      "  Cleaned up temp_DNase-bed\n",
      "Processing DNase-bigwig.zip...\n",
      "  Copied X1.bw -> X1/DNase_X1.bw\n",
      "  Copied X1.bw -> X1/DNase_X1.bw\n",
      "  Copied X2.bw -> X2/DNase_X2.bw\n",
      "  Copied X2.bw -> X2/DNase_X2.bw\n",
      "  Copied X3.bigwig -> X3/DNase_X3.bw\n",
      "  Cleaned up temp_DNase-bigwig\n",
      "Processing H3K27ac-bed.zip...\n",
      "  Copied X3.bigwig -> X3/DNase_X3.bw\n",
      "  Cleaned up temp_DNase-bigwig\n",
      "Processing H3K27ac-bed.zip...\n",
      "  Copied X1.bed -> X1/H3K27ac_X1.bed\n",
      "  Copied X2.bed -> X2/H3K27ac_X2.bed\n",
      "  Copied X3.bed -> X3/H3K27ac_X3.bed\n",
      "  Cleaned up temp_H3K27ac-bed\n",
      "Processing H3K27ac-bigwig.zip...\n",
      "  Copied X1.bed -> X1/H3K27ac_X1.bed\n",
      "  Copied X2.bed -> X2/H3K27ac_X2.bed\n",
      "  Copied X3.bed -> X3/H3K27ac_X3.bed\n",
      "  Cleaned up temp_H3K27ac-bed\n",
      "Processing H3K27ac-bigwig.zip...\n",
      "  Copied X1.bigwig -> X1/H3K27ac_X1.bw\n",
      "  Copied X1.bigwig -> X1/H3K27ac_X1.bw\n",
      "  Copied X2.bw -> X2/H3K27ac_X2.bw\n",
      "  Copied X2.bw -> X2/H3K27ac_X2.bw\n",
      "  Copied X3.bw -> X3/H3K27ac_X3.bw\n",
      "  Copied X3.bw -> X3/H3K27ac_X3.bw\n",
      "  Cleaned up temp_H3K27ac-bigwig\n",
      "Processing H3K27me3-bed.zip...\n",
      "  Copied X1.bed -> X1/H3K27me3_X1.bed\n",
      "  Copied X2.bed -> X2/H3K27me3_X2.bed\n",
      "  Cleaned up temp_H3K27ac-bigwig\n",
      "Processing H3K27me3-bed.zip...\n",
      "  Copied X1.bed -> X1/H3K27me3_X1.bed\n",
      "  Copied X2.bed -> X2/H3K27me3_X2.bed\n",
      "  Copied X3.bed -> X3/H3K27me3_X3.bed\n",
      "  Cleaned up temp_H3K27me3-bed\n",
      "Processing H3K27me3-bigwig.zip...\n",
      "  Copied X3.bed -> X3/H3K27me3_X3.bed\n",
      "  Cleaned up temp_H3K27me3-bed\n",
      "Processing H3K27me3-bigwig.zip...\n",
      "  Copied X1.bw -> X1/H3K27me3_X1.bw\n",
      "  Copied X1.bw -> X1/H3K27me3_X1.bw\n",
      "  Copied X2.bw -> X2/H3K27me3_X2.bw\n",
      "  Copied X2.bw -> X2/H3K27me3_X2.bw\n",
      "  Copied X3.bigwig -> X3/H3K27me3_X3.bw\n",
      "  Copied X3.bigwig -> X3/H3K27me3_X3.bw\n",
      "  Cleaned up temp_H3K27me3-bigwig\n",
      "Processing H3K36me3-bed.zip...\n",
      "  Cleaned up temp_H3K27me3-bigwig\n",
      "Processing H3K36me3-bed.zip...\n",
      "  Copied X1.bed -> X1/H3K36me3_X1.bed\n",
      "  Copied X2.bed -> X2/H3K36me3_X2.bed\n",
      "  Copied X3.bed -> X3/H3K36me3_X3.bed\n",
      "  Cleaned up temp_H3K36me3-bed\n",
      "Processing H3K36me3-bigwig.zip...\n",
      "  Copied X1.bed -> X1/H3K36me3_X1.bed\n",
      "  Copied X2.bed -> X2/H3K36me3_X2.bed\n",
      "  Copied X3.bed -> X3/H3K36me3_X3.bed\n",
      "  Cleaned up temp_H3K36me3-bed\n",
      "Processing H3K36me3-bigwig.zip...\n",
      "  Copied X1.bw -> X1/H3K36me3_X1.bw\n",
      "  Copied X1.bw -> X1/H3K36me3_X1.bw\n",
      "  Copied X2.bw -> X2/H3K36me3_X2.bw\n",
      "  Copied X2.bw -> X2/H3K36me3_X2.bw\n",
      "  Copied X3.bigwig -> X3/H3K36me3_X3.bw\n",
      "  Copied X3.bigwig -> X3/H3K36me3_X3.bw\n",
      "  Cleaned up temp_H3K36me3-bigwig\n",
      "Processing H3K4me1-bed.zip...\n",
      "  Cleaned up temp_H3K36me3-bigwig\n",
      "Processing H3K4me1-bed.zip...\n",
      "  Copied X1.bed -> X1/H3K4me1_X1.bed\n",
      "  Copied X2.bed -> X2/H3K4me1_X2.bed\n",
      "  Copied X3.bed -> X3/H3K4me1_X3.bed\n",
      "  Cleaned up temp_H3K4me1-bed\n",
      "Processing H3K4me1-bigwig.zip...\n",
      "  Copied X1.bed -> X1/H3K4me1_X1.bed\n",
      "  Copied X2.bed -> X2/H3K4me1_X2.bed\n",
      "  Copied X3.bed -> X3/H3K4me1_X3.bed\n",
      "  Cleaned up temp_H3K4me1-bed\n",
      "Processing H3K4me1-bigwig.zip...\n",
      "  Copied X1.bigwig -> X1/H3K4me1_X1.bw\n",
      "  Copied X1.bigwig -> X1/H3K4me1_X1.bw\n",
      "  Copied X2.bw -> X2/H3K4me1_X2.bw\n",
      "  Copied X2.bw -> X2/H3K4me1_X2.bw\n",
      "  Copied X3.bw -> X3/H3K4me1_X3.bw\n",
      "  Copied X3.bw -> X3/H3K4me1_X3.bw\n",
      "  Cleaned up temp_H3K4me1-bigwig\n",
      "Processing H3K4me3-bed.zip...\n",
      "  Cleaned up temp_H3K4me1-bigwig\n",
      "Processing H3K4me3-bed.zip...\n",
      "  Copied X1.bed -> X1/H3K4me3_X1.bed\n",
      "  Copied X2.bed -> X2/H3K4me3_X2.bed\n",
      "  Copied X3.bed -> X3/H3K4me3_X3.bed\n",
      "  Cleaned up temp_H3K4me3-bed\n",
      "Processing H3K4me3-bigwig.zip...\n",
      "  Copied X1.bed -> X1/H3K4me3_X1.bed\n",
      "  Copied X2.bed -> X2/H3K4me3_X2.bed\n",
      "  Copied X3.bed -> X3/H3K4me3_X3.bed\n",
      "  Cleaned up temp_H3K4me3-bed\n",
      "Processing H3K4me3-bigwig.zip...\n",
      "  Copied X1.bw -> X1/H3K4me3_X1.bw\n",
      "  Copied X1.bw -> X1/H3K4me3_X1.bw\n",
      "  Copied X2.bw -> X2/H3K4me3_X2.bw\n",
      "  Copied X2.bw -> X2/H3K4me3_X2.bw\n",
      "  Copied X3.bigwig -> X3/H3K4me3_X3.bw\n",
      "  Copied X3.bigwig -> X3/H3K4me3_X3.bw\n",
      "  Cleaned up temp_H3K4me3-bigwig\n",
      "Processing H3K9me3-bed.zip...\n",
      "  Cleaned up temp_H3K4me3-bigwig\n",
      "Processing H3K9me3-bed.zip...\n",
      "  Copied X1.bed -> X1/H3K9me3_X1.bed\n",
      "  Copied X2.bed -> X2/H3K9me3_X2.bed\n",
      "  Copied X3.bed -> X3/H3K9me3_X3.bed\n",
      "  Cleaned up temp_H3K9me3-bed\n",
      "Processing H3K9me3-bigwig.zip...\n",
      "  Copied X1.bed -> X1/H3K9me3_X1.bed\n",
      "  Copied X2.bed -> X2/H3K9me3_X2.bed\n",
      "  Copied X3.bed -> X3/H3K9me3_X3.bed\n",
      "  Cleaned up temp_H3K9me3-bed\n",
      "Processing H3K9me3-bigwig.zip...\n",
      "  Copied X1.bw -> X1/H3K9me3_X1.bw\n",
      "  Copied X1.bw -> X1/H3K9me3_X1.bw\n",
      "  Copied X2.bw -> X2/H3K9me3_X2.bw\n",
      "  Copied X2.bw -> X2/H3K9me3_X2.bw\n",
      "  Copied X3.bigwig -> X3/H3K9me3_X3.bw\n",
      "  Copied ._X1.bw -> X1/H3K9me3_X1.bw\n",
      "  Copied X3.bigwig -> X3/H3K9me3_X3.bw\n",
      "  Copied ._X1.bw -> X1/H3K9me3_X1.bw\n",
      "  Copied ._X2.bw -> X2/H3K9me3_X2.bw\n",
      "  Copied ._X2.bw -> X2/H3K9me3_X2.bw\n",
      "  Cleaned up temp_H3K9me3-bigwig\n",
      "\n",
      "All 15 zip files have been extracted and organized in data\n",
      "Structure: data/{X1,X2,X3}/{DataType}_{CellLine}.{bw,bed}\n",
      "  Cleaned up temp_H3K9me3-bigwig\n",
      "\n",
      "All 15 zip files have been extracted and organized in data\n",
      "Structure: data/{X1,X2,X3}/{DataType}_{CellLine}.{bw,bed}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "\n",
    "# Define paths\n",
    "source_dir = 'ML4G_Project_1_Data'  # Path to directory with zip files\n",
    "target_dir = 'data'  # Target directory for extracted files\n",
    "\n",
    "# Create target directory if it doesn't exist\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# Create folders for each cell line\n",
    "cell_lines = ['X1', 'X2', 'X3']\n",
    "for cell_line in cell_lines:\n",
    "    cell_line_dir = os.path.join(target_dir, cell_line)\n",
    "    os.makedirs(cell_line_dir, exist_ok=True)\n",
    "\n",
    "# Get all zip files in the source directory (excluding sample.zip)\n",
    "zip_files = [f for f in os.listdir(source_dir) if f.endswith('.zip') and f != 'sample.zip']\n",
    "\n",
    "# Process each zip file separately\n",
    "for zip_file in zip_files:\n",
    "    zip_path = os.path.join(source_dir, zip_file)\n",
    "    zip_name = os.path.splitext(zip_file)[0]\n",
    "    print(f\"Processing {zip_file}...\")\n",
    "    \n",
    "    # Extract to a unique temporary directory for this zip file\n",
    "    temp_extract_dir = os.path.join(target_dir, f'temp_{zip_name}')\n",
    "    os.makedirs(temp_extract_dir, exist_ok=True)\n",
    "    \n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(temp_extract_dir)\n",
    "    \n",
    "    # Walk through all extracted files from this zip and organize them\n",
    "    for root, dirs, files in os.walk(temp_extract_dir):\n",
    "        for file in files:\n",
    "            source_file = os.path.join(root, file)\n",
    "            \n",
    "            # Handle CAGE-train files separately\n",
    "            if 'CAGE-train' in root or file.endswith('.tsv'):\n",
    "                # Keep CAGE-train files in their own folder\n",
    "                cage_train_target = os.path.join(target_dir, 'CAGE-train')\n",
    "                os.makedirs(cage_train_target, exist_ok=True)\n",
    "                \n",
    "                # Preserve the relative path structure for CAGE files\n",
    "                rel_path = os.path.relpath(source_file, temp_extract_dir)\n",
    "                target_file = os.path.join(cage_train_target, rel_path)\n",
    "                os.makedirs(os.path.dirname(target_file), exist_ok=True)\n",
    "                shutil.copy2(source_file, target_file)\n",
    "                print(f\"  Copied {file} to CAGE-train/\")\n",
    "                continue\n",
    "            \n",
    "            # Determine which cell line this file belongs to\n",
    "            cell_line_found = False\n",
    "            for cell_line in cell_lines:\n",
    "                if cell_line in file:\n",
    "                    file_lower = file.lower()\n",
    "                    \n",
    "                    # Get file extension (handle both .bw and .bigwig)\n",
    "                    if file.endswith('.bw') or file.endswith('.bigwig'):\n",
    "                        ext = '.bw'\n",
    "                    elif file.endswith('.bed'):\n",
    "                        ext = '.bed'\n",
    "                    else:\n",
    "                        continue\n",
    "                    \n",
    "                    # Determine the mark/assay type from the zip file name\n",
    "                    zip_lower = zip_name.lower()\n",
    "                    if 'dnase' in zip_lower:\n",
    "                        data_type = 'DNase'\n",
    "                    elif 'h3k27ac' in zip_lower:\n",
    "                        data_type = 'H3K27ac'\n",
    "                    elif 'h3k27me3' in zip_lower:\n",
    "                        data_type = 'H3K27me3'\n",
    "                    elif 'h3k36me3' in zip_lower:\n",
    "                        data_type = 'H3K36me3'\n",
    "                    elif 'h3k4me1' in zip_lower:\n",
    "                        data_type = 'H3K4me1'\n",
    "                    elif 'h3k4me3' in zip_lower:\n",
    "                        data_type = 'H3K4me3'\n",
    "                    elif 'h3k9me3' in zip_lower:\n",
    "                        data_type = 'H3K9me3'\n",
    "                    else:\n",
    "                        # Skip if we can't identify the data type\n",
    "                        continue\n",
    "                    \n",
    "                    # Create new filename: {data_type}_{cell_line}{ext}\n",
    "                    new_filename = f\"{data_type}_{cell_line}{ext}\"\n",
    "                    target_file = os.path.join(target_dir, cell_line, new_filename)\n",
    "                    \n",
    "                    # Copy the file to the new location\n",
    "                    shutil.copy2(source_file, target_file)\n",
    "                    print(f\"  Copied {file} -> {cell_line}/{new_filename}\")\n",
    "                    cell_line_found = True\n",
    "                    break\n",
    "    \n",
    "    # Clean up this zip's temporary directory\n",
    "    shutil.rmtree(temp_extract_dir)\n",
    "    print(f\"  Cleaned up temp_{zip_name}\")\n",
    "\n",
    "print(f\"\\nAll {len(zip_files)} zip files have been extracted and organized in {target_dir}\")\n",
    "print(f\"Structure: data/{'{X1,X2,X3}'}/{'{DataType}_{CellLine}.{bw,bed}'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.1 - Modeling Choices & Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths etc.\n",
    "data_paths_bw = {\n",
    "   'X1': {\n",
    "        'DNase': 'data/X1/DNase_X1.bw',\n",
    "        'H3K27ac': 'data/X1/H3K27ac_X1.bw',\n",
    "        'H3K27me3': 'data/X1/H3K27me3_X1.bw',\n",
    "        'H3K36me3': 'data/X1/H3K36me3_X1.bw',\n",
    "        'H3K4me1': 'data/X1/H3K4me1_X1.bw',\n",
    "        'H3K4me3': 'data/X1/H3K4me3_X1.bw',\n",
    "        'H3K9me3': 'data/X1/H3K9me3_X1.bw'\n",
    "    },\n",
    "    'X2': {\n",
    "        'DNase': 'data/X2/DNase_X2.bw',\n",
    "        'H3K27ac': 'data/X2/H3K27ac_X2.bw',\n",
    "        'H3K27me3': 'data/X2/H3K27me3_X2.bw',\n",
    "        'H3K36me3': 'data/X2/H3K36me3_X2.bw',\n",
    "        'H3K4me1': 'data/X2/H3K4me1_X2.bw',\n",
    "        'H3K4me3': 'data/X2/H3K4me3_X2.bw',\n",
    "        'H3K9me3': 'data/X2/H3K9me3_X2.bw'\n",
    "    },\n",
    "    'X3': {\n",
    "        'DNase': 'data/X3/DNase_X3.bw',\n",
    "        'H3K27ac': 'data/X3/H3K27ac_X3.bw',\n",
    "        'H3K27me3': 'data/X3/H3K27me3_X3.bw',\n",
    "        'H3K36me3': 'data/X3/H3K36me3_X3.bw',\n",
    "        'H3K4me1': 'data/X3/H3K4me1_X3.bw',\n",
    "        'H3K4me3': 'data/X3/H3K4me3_X3.bw',\n",
    "        'H3K9me3': 'data/X3/H3K9me3_X3.bw'\n",
    "    }\n",
    "}\n",
    "\n",
    "data_paths_bed = {\n",
    "     'X1': {\n",
    "        'DNase': 'data/X1/DNase_X1.bed',\n",
    "        'H3K27ac': 'data/X1/H3K27ac_X1.bed',\n",
    "        'H3K27me3': 'data/X1/H3K27me3_X1.bed',\n",
    "        'H3K36me3': 'data/X1/H3K36me3_X1.bed',\n",
    "        'H3K4me1': 'data/X1/H3K4me1_X1.bed',\n",
    "        'H3K4me3': 'data/X1/H3K4me3_X1.bed',\n",
    "        'H3K9me3': 'data/X1/H3K9me3_X1.bed'\n",
    "    },\n",
    "    'X2': {\n",
    "        'DNase': 'data/X2/DNase_X2.bed',\n",
    "        'H3K27ac': 'data/X2/H3K27ac_X2.bed',\n",
    "        'H3K27me3': 'data/X2/H3K27me3_X2.bed',\n",
    "        'H3K36me3': 'data/X2/H3K36me3_X2.bed',\n",
    "        'H3K4me1': 'data/X2/H3K4me1_X2.bed',\n",
    "        'H3K4me3': 'data/X2/H3K4me3_X2.bed',\n",
    "        'H3K9me3': 'data/X2/H3K9me3_X2.bed'\n",
    "    },\n",
    "    'X3': {\n",
    "        'DNase': 'data/X3/DNase_X3.bed',\n",
    "        'H3K27ac': 'data/X3/H3K27ac_X3.bed',\n",
    "        'H3K27me3': 'data/X3/H3K27me3_X3.bed',\n",
    "        'H3K36me3': 'data/X3/H3K36me3_X3.bed',\n",
    "        'H3K4me1': 'data/X3/H3K4me1_X3.bed',\n",
    "        'H3K4me3': 'data/X3/H3K4me3_X3.bed',\n",
    "        'H3K9me3': 'data/X3/H3K9me3_X3.bed'\n",
    "    }\n",
    "}\n",
    "\n",
    "gene_paths = {\n",
    "    'X1': {\n",
    "        'train':{\n",
    "            'info': 'data/CAGE-train/CAGE-train/X1_train_info.tsv',\n",
    "            'target': 'data/CAGE-train/CAGE-train/X1_train_y.tsv'\n",
    "        },\n",
    "        'validation':{\n",
    "            'info': 'data/CAGE-train/CAGE-train/X1_val_info.tsv',\n",
    "            'target': 'data/CAGE-train/CAGE-train/X1_val_y.tsv'\n",
    "        }\n",
    "    },\n",
    "    'X2': {\n",
    "        'train':{\n",
    "            'info': 'data/CAGE-train/CAGE-train/X2_train_info.tsv',\n",
    "            'target': 'data/CAGE-train/CAGE-train/X2_train_y.tsv'\n",
    "        },\n",
    "        'validation':{\n",
    "            'info': 'data/CAGE-train/CAGE-train/X2_val_info.tsv',\n",
    "            'target': 'data/CAGE-train/CAGE-train/X2_val_y.tsv'\n",
    "        }\n",
    "    },\n",
    "    'X3': 'X3_test_info.tsv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Use bw files, compute average, min, max, std over a region that spans the transcription start site (TSS) +/- 10kb\n",
    "import pyBigWig\n",
    "\n",
    "# Idea: Function that given cell line and type (train/val) extracts features and targets\n",
    "def get_dataset(cell_line, set_type):\n",
    "\n",
    "    gene_info_df = pd.read_csv(gene_paths[cell_line][set_type]['info'], sep='\\t')\n",
    "    gene_target_df = pd.read_csv(gene_paths[cell_line][set_type]['target'], sep='\\t')\n",
    "    \n",
    "    features = extract_all_features(gene_info_df, cell_line)\n",
    "    targets = gene_target_df['gex'].values\n",
    "\n",
    "    return features, targets\n",
    "\n",
    "def extract_all_features(gene_info_df, cell_line, window=10000, bw_paths=None):\n",
    "    \"\"\"\n",
    "    Extract summary statistics from bigWig files for each gene in `gene_info_df` and\n",
    "    each mark defined in `bw_paths` (or `data_paths_bw[cell_line]` by default).\n",
    "\n",
    "    Parameters:\n",
    "    - gene_info_df: pd.DataFrame with at least columns ['chr', 'tss'] and optionally 'gene_name'.\n",
    "    - cell_line: one of the keys in data_paths_bw (e.g. 'X1').\n",
    "    - window: int, number of bases upstream/downstream of TSS to include (default 10_000).\n",
    "    - bw_paths: optional dict of mark->path. If None, uses data_paths_bw[cell_line].\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: index matches gene order (uses 'gene_name' if present), columns like '<MARK>_avg', '<MARK>_min', '<MARK>_max', '<MARK>_std'.\n",
    "    \"\"\"\n",
    "    if bw_paths is None:\n",
    "        bw_paths = data_paths_bw.get(cell_line, {})\n",
    "\n",
    "    # Prepare index / gene identifiers\n",
    "    if 'gene_name' in gene_info_df.columns:\n",
    "        gene_names = gene_info_df['gene_name'].astype(str).tolist()\n",
    "    else:\n",
    "        gene_names = gene_info_df.index.astype(str).tolist()\n",
    "\n",
    "    feature_frames = []\n",
    "\n",
    "    for mark, path in bw_paths.items():\n",
    "        print(f\"Extracting features for {mark} from {path}...\")\n",
    "\n",
    "        # Default fallback columns for this mark (will keep gene order)\n",
    "        cols = [f\"{mark}_avg\", f\"{mark}_min\", f\"{mark}_max\", f\"{mark}_std\"]\n",
    "        rows = []\n",
    "\n",
    "        # Check file exists\n",
    "        try:\n",
    "            bw = pyBigWig.open(path)\n",
    "            if bw is None:\n",
    "                raise FileNotFoundError(f\"Could not open bigWig: {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: cannot open {path}: {e}. Filling zeros for this mark.\")\n",
    "            # fill zeros for all genes if the file can't be opened\n",
    "            rows = [[0.0, 0.0, 0.0, 0.0] for _ in range(len(gene_names))]\n",
    "            feature_frames.append(pd.DataFrame(rows, columns=cols, index=gene_names))\n",
    "            continue\n",
    "\n",
    "        # iterate genes in order and extract statistics\n",
    "        for _, row in gene_info_df.iterrows():\n",
    "            chrom = str(row['chr'])\n",
    "            \n",
    "            start = max(0, int(row['TSS_start']) - window)\n",
    "            end = int(row['TSS_end']) + window\n",
    "            try:\n",
    "                values = bw.values(chrom, start, end, numpy=True)\n",
    "                # pyBigWig may return None if region is invalid\n",
    "                if values is None:\n",
    "                    vals = np.array([], dtype=float)\n",
    "                else:\n",
    "                    vals = np.asarray(values, dtype=float)\n",
    "                # replace NaNs and handle empty arrays\n",
    "                if vals.size == 0:\n",
    "                    rows.append([0.0, 0.0, 0.0, 0.0])\n",
    "                else:\n",
    "                    vals = np.nan_to_num(vals, nan=0.0)\n",
    "                    rows.append([float(np.mean(vals)), float(np.min(vals)), float(np.max(vals)), float(np.std(vals))])\n",
    "            except Exception as e:\n",
    "                # keep alignment with genes even when extraction fails for a gene\n",
    "                print(f\"  Warning: failed for {chrom}:{start}-{end} ({e}). Using zeros.\")\n",
    "                rows.append([0.0, 0.0, 0.0, 0.0])\n",
    "\n",
    "        # close bigWig handle\n",
    "        try:\n",
    "            bw.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # create dataframe for this mark, use gene_names as index to preserve order\n",
    "        mark_df = pd.DataFrame(rows, columns=cols, index=gene_names)\n",
    "        feature_frames.append(mark_df)\n",
    "\n",
    "    # concat horizontally and ensure index is gene names\n",
    "    if feature_frames:\n",
    "        all_features_df = pd.concat(feature_frames, axis=1)\n",
    "        all_features_df.index.name = 'gene_name'\n",
    "    else:\n",
    "        all_features_df = pd.DataFrame(index=gene_names)\n",
    "\n",
    "    return all_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for DNase from data/X1/DNase_X1.bw...\n",
      "Extracting features for H3K27ac from data/X1/H3K27ac_X1.bw...\n",
      "Extracting features for H3K27me3 from data/X1/H3K27me3_X1.bw...\n",
      "Extracting features for H3K36me3 from data/X1/H3K36me3_X1.bw...\n",
      "Extracting features for H3K4me1 from data/X1/H3K4me1_X1.bw...\n",
      "Extracting features for H3K4me3 from data/X1/H3K4me3_X1.bw...\n",
      "Extracting features for H3K9me3 from data/X1/H3K9me3_X1.bw...\n",
      "           DNase_avg  DNase_min  DNase_max  DNase_std  H3K27ac_avg  \\\n",
      "gene_name                                                            \n",
      "SLC20A1     0.120745   0.024102   0.929387   0.114712     1.100506   \n",
      "C11orf58    0.145684   0.024102   4.175937   0.437137     4.124743   \n",
      "ZSCAN9      0.114666   0.024102   2.427793   0.232137     1.081377   \n",
      "CD19        0.355132   0.024102   3.083350   0.541315     9.234150   \n",
      "TMEM123     0.202145   0.024102   2.615094   0.321339     2.547052   \n",
      "\n",
      "           H3K27ac_min  H3K27ac_max  H3K27ac_std  H3K27me3_avg  H3K27me3_min  \\\n",
      "gene_name                                                                      \n",
      "SLC20A1        0.00108    16.681410     2.114651      0.257314           0.0   \n",
      "C11orf58       0.00803   117.568718    15.139802      0.491965           0.0   \n",
      "ZSCAN9         0.00108    24.873249     3.255718      0.833538           0.0   \n",
      "CD19           0.02218   132.755112    20.793088      0.174909           0.0   \n",
      "TMEM123        0.00131    38.896629     6.107036      0.414539           0.0   \n",
      "\n",
      "           ...  H3K4me1_max  H3K4me1_std  H3K4me3_avg  H3K4me3_min  \\\n",
      "gene_name  ...                                                       \n",
      "SLC20A1    ...      6.14403     0.964853     0.995666          0.0   \n",
      "C11orf58   ...      7.09992     0.898545     5.678698          0.0   \n",
      "ZSCAN9     ...      6.14403     0.767536     3.768475          0.0   \n",
      "CD19       ...     16.03422     1.821573     9.581315          0.0   \n",
      "TMEM123    ...      8.86640     1.386689     6.178401          0.0   \n",
      "\n",
      "           H3K4me3_max  H3K4me3_std  H3K9me3_avg  H3K9me3_min  H3K9me3_max  \\\n",
      "gene_name                                                                    \n",
      "SLC20A1      20.580021     2.315785     0.359837      0.00803      2.35659   \n",
      "C11orf58     81.762840    14.861309     0.457902      0.00294      2.19785   \n",
      "ZSCAN9       82.180740    11.475345     0.394788      0.00294      1.80598   \n",
      "CD19        111.197151    20.784757     0.356641      0.00294      2.74418   \n",
      "TMEM123      67.215988    13.209354     0.597877      0.00040      4.94804   \n",
      "\n",
      "           H3K9me3_std  \n",
      "gene_name               \n",
      "SLC20A1       0.252406  \n",
      "C11orf58      0.321351  \n",
      "ZSCAN9        0.278262  \n",
      "CD19          0.307851  \n",
      "TMEM123       0.582921  \n",
      "\n",
      "[5 rows x 28 columns]\n",
      "[   0.         2239.1033284    19.7980636   411.5306229    34.21412932]\n"
     ]
    }
   ],
   "source": [
    "X, y = get_dataset('X1', 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict():\n",
    "    data_dict = {\n",
    "        'X1': {\n",
    "            'train': get_dataset('X1', 'train'),\n",
    "            'validation': get_dataset('X1', 'validation')\n",
    "        },\n",
    "        'X2': {\n",
    "            'train': get_dataset('X2', 'train'),\n",
    "            'validation': get_dataset('X2', 'validation')\n",
    "        }\n",
    "    }\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_dict):\n",
    "    \"\"\" The pipeline goes something like this:\n",
    "    1) We get all the data (X1 train, X1 Validation, X2 train, X2 validation)\n",
    "    2) Then we try the following combinations:\n",
    "        a) Train model on X1 train, validate on X1 validation and X2 validation\n",
    "        b) Train model on X2 train, validate on X2 validation and X1 validation\n",
    "        c) Train model on X1 + X2 train, validate on X1 validation and X2 validation    \n",
    "    \"\"\"\n",
    "\n",
    "    results = {}\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    for train_cell_line in ['X1', 'X2', ['X1', 'X2']]:\n",
    "        # Train on single cell line\n",
    "        if isinstance(train_cell_line, list):\n",
    "            X_train = np.concatenate([data_dict[cell]['train'][0] for cell in train_cell_line])\n",
    "            y_train = np.concatenate([data_dict[cell]['train'][1] for cell in train_cell_line])\n",
    "        else:\n",
    "            X_train, y_train = data_dict[train_cell_line]['train']\n",
    "        \n",
    "        model.fit(scaler.fit_transform(X_train), y_train)\n",
    "        \n",
    "        for val_cell_line in ['X1', 'X2']:\n",
    "            X_val, y_val = data_dict[val_cell_line]['validation']\n",
    "            X_val_scaled = scaler.transform(X_val)\n",
    "            y_val_pred = model.predict(X_val_scaled)\n",
    "            \n",
    "            mse = mean_squared_error(y_val, y_val_pred)\n",
    "            r2 = r2_score(y_val, y_val_pred)\n",
    "            spearmanr_corr, _ = spearmanr(y_val, y_val_pred)\n",
    "\n",
    "            results[(str(train_cell_line), str(val_cell_line))] = {\n",
    "                'MSE': mse,\n",
    "                'R^2': r2,\n",
    "                \"Spearman's rho\": spearmanr_corr\n",
    "            }\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting features for DNase from data/X1/DNase_X1.bw...\n",
      "Extracting features for H3K27ac from data/X1/H3K27ac_X1.bw...\n",
      "Extracting features for H3K27me3 from data/X1/H3K27me3_X1.bw...\n",
      "Extracting features for H3K36me3 from data/X1/H3K36me3_X1.bw...\n",
      "Extracting features for H3K4me1 from data/X1/H3K4me1_X1.bw...\n",
      "Extracting features for H3K4me3 from data/X1/H3K4me3_X1.bw...\n",
      "Extracting features for H3K9me3 from data/X1/H3K9me3_X1.bw...\n",
      "Extracting features for DNase from data/X1/DNase_X1.bw...\n",
      "Extracting features for H3K27ac from data/X1/H3K27ac_X1.bw...\n",
      "Extracting features for H3K27me3 from data/X1/H3K27me3_X1.bw...\n",
      "Extracting features for H3K36me3 from data/X1/H3K36me3_X1.bw...\n",
      "Extracting features for H3K4me1 from data/X1/H3K4me1_X1.bw...\n",
      "Extracting features for H3K4me3 from data/X1/H3K4me3_X1.bw...\n",
      "Extracting features for H3K9me3 from data/X1/H3K9me3_X1.bw...\n",
      "Extracting features for DNase from data/X2/DNase_X2.bw...\n",
      "Extracting features for H3K27ac from data/X2/H3K27ac_X2.bw...\n",
      "Extracting features for H3K27me3 from data/X2/H3K27me3_X2.bw...\n",
      "Extracting features for H3K36me3 from data/X2/H3K36me3_X2.bw...\n",
      "Extracting features for H3K4me1 from data/X2/H3K4me1_X2.bw...\n",
      "Extracting features for H3K4me3 from data/X2/H3K4me3_X2.bw...\n",
      "Extracting features for H3K9me3 from data/X2/H3K9me3_X2.bw...\n",
      "Extracting features for DNase from data/X2/DNase_X2.bw...\n",
      "Extracting features for H3K27ac from data/X2/H3K27ac_X2.bw...\n",
      "Extracting features for H3K27me3 from data/X2/H3K27me3_X2.bw...\n",
      "Extracting features for H3K36me3 from data/X2/H3K36me3_X2.bw...\n",
      "Extracting features for H3K4me1 from data/X2/H3K4me1_X2.bw...\n",
      "Extracting features for H3K4me3 from data/X2/H3K4me3_X2.bw...\n",
      "Extracting features for H3K9me3 from data/X2/H3K9me3_X2.bw...\n",
      "Evaluating Linear Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janic/miniconda3/envs/ml4g/lib/python3.10/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model_name, model \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEvaluating \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (train_cell_line, val_cell_line), metrics \u001b[38;5;129;01min\u001b[39;00m results\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     14\u001b[0m         mse \u001b[38;5;241m=\u001b[39m metrics[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[21], line 46\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, data_dict)\u001b[0m\n\u001b[1;32m     43\u001b[0m         r2 \u001b[38;5;241m=\u001b[39m r2_score(y_val, y_val_pred)\n\u001b[1;32m     44\u001b[0m         spearmanr_corr, _ \u001b[38;5;241m=\u001b[39m spearmanr(y_val, y_val_pred)\n\u001b[0;32m---> 46\u001b[0m         \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_cell_line\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_cell_line\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     47\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m: mse,\n\u001b[1;32m     48\u001b[0m             \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR^2\u001b[39m\u001b[38;5;124m'\u001b[39m: r2,\n\u001b[1;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSpearman\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms rho\u001b[39m\u001b[38;5;124m\"\u001b[39m: spearmanr_corr\n\u001b[1;32m     50\u001b[0m         }\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'list'"
     ]
    }
   ],
   "source": [
    "data_dict = get_data_dict()\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Linear Regression...\n",
      "  Trained on X1, validated on X1: MSE=82660.0225, R^2=0.0473, Spearman's rho=0.5504\n",
      "  Trained on X1, validated on X2: MSE=72527.6524, R^2=0.0060, Spearman's rho=0.4331\n",
      "  Trained on X2, validated on X1: MSE=83889.4926, R^2=0.0332, Spearman's rho=0.4691\n",
      "  Trained on X2, validated on X2: MSE=68014.3090, R^2=0.0678, Spearman's rho=0.4598\n",
      "  Trained on ['X1', 'X2'], validated on X1: MSE=81185.9989, R^2=0.0643, Spearman's rho=0.5453\n",
      "  Trained on ['X1', 'X2'], validated on X2: MSE=68727.0658, R^2=0.0581, Spearman's rho=0.4996\n",
      "\n",
      "Evaluating Ridge Regression...\n",
      "  Trained on X1, validated on X1: MSE=82659.6599, R^2=0.0473, Spearman's rho=0.5504\n",
      "  Trained on X1, validated on X2: MSE=72530.9050, R^2=0.0059, Spearman's rho=0.4333\n",
      "  Trained on X2, validated on X1: MSE=83874.1543, R^2=0.0333, Spearman's rho=0.4694\n",
      "  Trained on X2, validated on X2: MSE=68015.2145, R^2=0.0678, Spearman's rho=0.4600\n",
      "  Trained on ['X1', 'X2'], validated on X1: MSE=81184.6267, R^2=0.0643, Spearman's rho=0.5454\n",
      "  Trained on ['X1', 'X2'], validated on X2: MSE=68727.7377, R^2=0.0581, Spearman's rho=0.4997\n",
      "\n",
      "Evaluating Lasso Regression...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janic/miniconda3/envs/ml4g/lib/python3.10/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/janic/miniconda3/envs/ml4g/lib/python3.10/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/janic/miniconda3/envs/ml4g/lib/python3.10/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/janic/miniconda3/envs/ml4g/lib/python3.10/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/janic/miniconda3/envs/ml4g/lib/python3.10/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/janic/miniconda3/envs/ml4g/lib/python3.10/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Trained on X1, validated on X1: MSE=82658.2743, R^2=0.0474, Spearman's rho=0.5505\n",
      "  Trained on X1, validated on X2: MSE=72411.3779, R^2=0.0076, Spearman's rho=0.4453\n",
      "  Trained on X2, validated on X1: MSE=83120.2717, R^2=0.0420, Spearman's rho=0.4762\n",
      "  Trained on X2, validated on X2: MSE=68075.3723, R^2=0.0670, Spearman's rho=0.4646\n",
      "  Trained on ['X1', 'X2'], validated on X1: MSE=81094.6037, R^2=0.0654, Spearman's rho=0.5481\n",
      "  Trained on ['X1', 'X2'], validated on X2: MSE=68744.2164, R^2=0.0578, Spearman's rho=0.5059\n",
      "\n",
      "Evaluating Random Forest...\n",
      "  Trained on X1, validated on X1: MSE=113119.3937, R^2=-0.3037, Spearman's rho=0.5837\n",
      "  Trained on X1, validated on X2: MSE=500145.7494, R^2=-5.8546, Spearman's rho=0.4877\n",
      "  Trained on X2, validated on X1: MSE=526814.3827, R^2=-5.0716, Spearman's rho=0.5657\n",
      "  Trained on X2, validated on X2: MSE=79335.4166, R^2=-0.0873, Spearman's rho=0.5701\n",
      "  Trained on ['X1', 'X2'], validated on X1: MSE=93014.0180, R^2=-0.0720, Spearman's rho=0.5853\n",
      "  Trained on ['X1', 'X2'], validated on X2: MSE=78473.9361, R^2=-0.0755, Spearman's rho=0.5753\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/janic/miniconda3/envs/ml4g/lib/python3.10/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n",
      "/home/janic/miniconda3/envs/ml4g/lib/python3.10/site-packages/sklearn/utils/validation.py:2742: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for model_name, model in models.items():\n",
    "    print(f\"Evaluating {model_name}...\")\n",
    "    results = evaluate_model(model, data_dict)\n",
    "    for (train_cell_line, val_cell_line), metrics in results.items():\n",
    "        mse = metrics[\"MSE\"]\n",
    "        r2 = metrics[\"R^2\"]\n",
    "        spearman_rho = metrics[\"Spearman's rho\"]\n",
    "        print(f\"  Trained on {train_cell_line}, validated on {val_cell_line}: MSE={mse:.4f}, R^2={r2:.4f}, Spearman's rho={spearman_rho:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# Load your feature (bed and/or bigwig and/or fasta) and target files (tsv) here.\n",
    "# Decide which features to use for training. Feel free to process them however you need.\n",
    "\n",
    "# NOTE: \n",
    "# bed and bigwig files contain signals of all chromosomes (including sex chromosomes).\n",
    "# Training and validation split based on chromosomes has been done for you. \n",
    "# However, you can resplit the data in any way you want.\n",
    "\n",
    "path_data = \"data\\\\CAGE-train\\\\CAGE-train\" \n",
    "path_test = \"data\\\\CAGE-train\\\\CAGE-train\\\\X3_test_info.tsv\"\n",
    "test_genes = pd.read_csv(path_test, sep='\\t')\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------------------- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.2 - Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: \n",
    "# Select the best model to predict gene expression from the obtained features in WP 1.1.\n",
    "\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work Package 1.3 - Prediction on Test Data (Evaluation Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO:\n",
    "# Using the model trained in WP 1.2, make predictions on the test data (chr 1 of cell line X3).\n",
    "# Store predictions in a variable called \"pred\" which is a numpy array.\n",
    "pred: np.ndarray\n",
    "pred = np.array([])  # TODO\n",
    "# ---------------------------INSERT CODE HERE---------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------\n",
    "\n",
    "# Check if \"pred\" meets the specified constrains\n",
    "assert isinstance(pred, np.ndarray), 'Prediction array must be a numpy array'\n",
    "assert np.issubdtype(pred.dtype, np.number), 'Prediction array must be numeric'\n",
    "assert pred.shape[0] == len(test_genes), 'Each gene should have a unique predicted expression'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store Predictions in the Required Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store predictions in a ZIP. \n",
    "# Upload this zip on the project website under \"Your submission\".\n",
    "# Zip this notebook along with the conda environment (and README, optional) and upload this under \"Your code\".\n",
    "\n",
    "save_dir = 'path/to/save/output/file'  # TODO\n",
    "file_name = 'gex_predicted.csv'         # PLEASE DO NOT CHANGE THIS\n",
    "zip_name = \"LastName_FirstName_Project1.zip\" # TODO\n",
    "save_path = f'{save_dir}/{zip_name}'\n",
    "compression_options = dict(method=\"zip\", archive_name=file_name)\n",
    "\n",
    "test_genes['gex_predicted'] = pred.tolist()\n",
    "test_genes[['gene_name', 'gex_predicted']].to_csv(save_path, compression=compression_options)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml4g",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
