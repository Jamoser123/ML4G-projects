{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c692f268",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.stats import skew\n",
    "from scipy.stats import kurtosis\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import pybigtools\n",
    "from joblib import Parallel, delayed\n",
    "import multiprocessing\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, r_regression\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, Matern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630c38bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unzip and organize data files\n",
    "\n",
    "unzip = False\n",
    "\n",
    "if unzip:\n",
    "\n",
    "    # Define paths\n",
    "    source_dir = 'ML4G_Project_1_Data'  # Path to directory with zip files\n",
    "    target_dir = 'data'  # Target directory for extracted files\n",
    "\n",
    "    # Create target directory if it doesn't exist\n",
    "    os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "    # Create folders for each cell line\n",
    "    cell_lines = ['X1', 'X2', 'X3']\n",
    "    for cell_line in cell_lines:\n",
    "        cell_line_dir = os.path.join(target_dir, cell_line)\n",
    "        os.makedirs(cell_line_dir, exist_ok=True)\n",
    "\n",
    "    # Get all zip files in the source directory (excluding sample.zip)\n",
    "    zip_files = [f for f in os.listdir(source_dir) if f.endswith('.zip') and f != 'sample.zip']\n",
    "\n",
    "    # Process each zip file separately\n",
    "    for zip_file in zip_files:\n",
    "        zip_path = os.path.join(source_dir, zip_file)\n",
    "        zip_name = os.path.splitext(zip_file)[0]\n",
    "        print(f\"Processing {zip_file}...\")\n",
    "        \n",
    "        # Extract to a unique temporary directory for this zip file\n",
    "        temp_extract_dir = os.path.join(target_dir, f'temp_{zip_name}')\n",
    "        os.makedirs(temp_extract_dir, exist_ok=True)\n",
    "        \n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            zip_ref.extractall(temp_extract_dir)\n",
    "        \n",
    "        # Walk through all extracted files from this zip and organize them\n",
    "        for root, dirs, files in os.walk(temp_extract_dir):\n",
    "            if 'MACOSX' in root:\n",
    "                continue\n",
    "            for file in files:\n",
    "                source_file = os.path.join(root, file)\n",
    "                \n",
    "                # Handle CAGE-train files separately\n",
    "                if 'CAGE-train' in root or file.endswith('.tsv'):\n",
    "                    # Keep CAGE-train files in their own folder\n",
    "                    cage_train_target = os.path.join(target_dir, 'CAGE-train')\n",
    "                    os.makedirs(cage_train_target, exist_ok=True)\n",
    "                    \n",
    "                    # Preserve the relative path structure for CAGE files\n",
    "                    rel_path = os.path.relpath(source_file, temp_extract_dir)\n",
    "                    target_file = os.path.join(cage_train_target, rel_path)\n",
    "                    os.makedirs(os.path.dirname(target_file), exist_ok=True)\n",
    "                    shutil.copy2(source_file, target_file)\n",
    "                    print(f\"  Copied {file} to CAGE-train/\")\n",
    "                    continue\n",
    "                \n",
    "                # Determine which cell line this file belongs to\n",
    "                cell_line_found = False\n",
    "                for cell_line in cell_lines:\n",
    "                    if cell_line in file:\n",
    "                        file_lower = file.lower()\n",
    "                        \n",
    "                        # Get file extension (handle both .bw and .bigwig)\n",
    "                        if file.endswith('.bw') or file.endswith('.bigwig'):\n",
    "                            ext = '.bw'\n",
    "                        elif file.endswith('.bed'):\n",
    "                            ext = '.bed'\n",
    "                        else:\n",
    "                            continue\n",
    "                        \n",
    "                        # Determine the mark/assay type from the zip file name\n",
    "                        zip_lower = zip_name.lower()\n",
    "                        if 'dnase' in zip_lower:\n",
    "                            data_type = 'DNase'\n",
    "                        elif 'h3k27ac' in zip_lower:\n",
    "                            data_type = 'H3K27ac'\n",
    "                        elif 'h3k27me3' in zip_lower:\n",
    "                            data_type = 'H3K27me3'\n",
    "                        elif 'h3k36me3' in zip_lower:\n",
    "                            data_type = 'H3K36me3'\n",
    "                        elif 'h3k4me1' in zip_lower:\n",
    "                            data_type = 'H3K4me1'\n",
    "                        elif 'h3k4me3' in zip_lower:\n",
    "                            data_type = 'H3K4me3'\n",
    "                        elif 'h3k9me3' in zip_lower:\n",
    "                            data_type = 'H3K9me3'\n",
    "                        else:\n",
    "                            # Skip if we can't identify the data type\n",
    "                            continue\n",
    "                        \n",
    "                        # Create new filename: {data_type}_{cell_line}{ext}\n",
    "                        new_filename = f\"{data_type}_{cell_line}{ext}\"\n",
    "                        target_file = os.path.join(target_dir, cell_line, new_filename)\n",
    "                        \n",
    "                        # Copy the file to the new location\n",
    "                        shutil.copy2(source_file, target_file)\n",
    "                        print(f\"  Copied {file} -> {cell_line}/{new_filename}\")\n",
    "                        cell_line_found = True\n",
    "                        break\n",
    "        \n",
    "        # Clean up this zip's temporary directory\n",
    "        shutil.rmtree(temp_extract_dir)\n",
    "        print(f\"  Cleaned up temp_{zip_name}\")\n",
    "\n",
    "    print(f\"\\nAll {len(zip_files)} zip files have been extracted and organized in {target_dir}\")\n",
    "    print(f\"Structure: data/{'{X1,X2,X3}'}/{'{DataType}_{CellLine}.{bw,bed}'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4287906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths etc.\n",
    "data_paths_bw = {\n",
    "   'X1': {\n",
    "        'DNase': 'data/X1/DNase_X1.bw',\n",
    "        'H3K27ac': 'data/X1/H3K27ac_X1.bw',\n",
    "        'H3K27me3': 'data/X1/H3K27me3_X1.bw',\n",
    "        'H3K36me3': 'data/X1/H3K36me3_X1.bw',\n",
    "        'H3K4me1': 'data/X1/H3K4me1_X1.bw',\n",
    "        'H3K4me3': 'data/X1/H3K4me3_X1.bw',\n",
    "        'H3K9me3': 'data/X1/H3K9me3_X1.bw'\n",
    "    },\n",
    "    'X2': {\n",
    "        'DNase': 'data/X2/DNase_X2.bw',\n",
    "        'H3K27ac': 'data/X2/H3K27ac_X2.bw',\n",
    "        'H3K27me3': 'data/X2/H3K27me3_X2.bw',\n",
    "        'H3K36me3': 'data/X2/H3K36me3_X2.bw',\n",
    "        'H3K4me1': 'data/X2/H3K4me1_X2.bw',\n",
    "        'H3K4me3': 'data/X2/H3K4me3_X2.bw',\n",
    "        'H3K9me3': 'data/X2/H3K9me3_X2.bw'\n",
    "    },\n",
    "    'X3': {\n",
    "        'DNase': 'data/X3/DNase_X3.bw',\n",
    "        'H3K27ac': 'data/X3/H3K27ac_X3.bw',\n",
    "        'H3K27me3': 'data/X3/H3K27me3_X3.bw',\n",
    "        'H3K36me3': 'data/X3/H3K36me3_X3.bw',\n",
    "        'H3K4me1': 'data/X3/H3K4me1_X3.bw',\n",
    "        'H3K4me3': 'data/X3/H3K4me3_X3.bw',\n",
    "        'H3K9me3': 'data/X3/H3K9me3_X3.bw'\n",
    "    }\n",
    "}\n",
    "\n",
    "data_paths_bed = {\n",
    "     'X1': {\n",
    "        'DNase': 'data/X1/DNase_X1.bed',\n",
    "        'H3K27ac': 'data/X1/H3K27ac_X1.bed',\n",
    "        'H3K27me3': 'data/X1/H3K27me3_X1.bed',\n",
    "        'H3K36me3': 'data/X1/H3K36me3_X1.bed',\n",
    "        'H3K4me1': 'data/X1/H3K4me1_X1.bed',\n",
    "        'H3K4me3': 'data/X1/H3K4me3_X1.bed',\n",
    "        'H3K9me3': 'data/X1/H3K9me3_X1.bed'\n",
    "    },\n",
    "    'X2': {\n",
    "        'DNase': 'data/X2/DNase_X2.bed',\n",
    "        'H3K27ac': 'data/X2/H3K27ac_X2.bed',\n",
    "        'H3K27me3': 'data/X2/H3K27me3_X2.bed',\n",
    "        'H3K36me3': 'data/X2/H3K36me3_X2.bed',\n",
    "        'H3K4me1': 'data/X2/H3K4me1_X2.bed',\n",
    "        'H3K4me3': 'data/X2/H3K4me3_X2.bed',\n",
    "        'H3K9me3': 'data/X2/H3K9me3_X2.bed'\n",
    "    },\n",
    "    'X3': {\n",
    "        'DNase': 'data/X3/DNase_X3.bed',\n",
    "        'H3K27ac': 'data/X3/H3K27ac_X3.bed',\n",
    "        'H3K27me3': 'data/X3/H3K27me3_X3.bed',\n",
    "        'H3K36me3': 'data/X3/H3K36me3_X3.bed',\n",
    "        'H3K4me1': 'data/X3/H3K4me1_X3.bed',\n",
    "        'H3K4me3': 'data/X3/H3K4me3_X3.bed',\n",
    "        'H3K9me3': 'data/X3/H3K9me3_X3.bed'\n",
    "    }\n",
    "}\n",
    "\n",
    "gene_paths = {\n",
    "    'X1': {\n",
    "        'train':{\n",
    "            'info': 'data/CAGE-train/CAGE-train/X1_train_info.tsv',\n",
    "            'target': 'data/CAGE-train/CAGE-train/X1_train_y.tsv'\n",
    "        },\n",
    "        'validation':{\n",
    "            'info': 'data/CAGE-train/CAGE-train/X1_val_info.tsv',\n",
    "            'target': 'data/CAGE-train/CAGE-train/X1_val_y.tsv'\n",
    "        }\n",
    "    },\n",
    "    'X2': {\n",
    "        'train':{\n",
    "            'info': 'data/CAGE-train/CAGE-train/X2_train_info.tsv',\n",
    "            'target': 'data/CAGE-train/CAGE-train/X2_train_y.tsv'\n",
    "        },\n",
    "        'validation':{\n",
    "            'info': 'data/CAGE-train/CAGE-train/X2_val_info.tsv',\n",
    "            'target': 'data/CAGE-train/CAGE-train/X2_val_y.tsv'\n",
    "        }\n",
    "    },\n",
    "    'X3': 'X3_test_info.tsv'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff632afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_features(vals, wide_values=None, weights=None, num_bins=10, bin_stats=['mean']):\n",
    "    \"\"\"\n",
    "    Calculate all features for a given signal array.\n",
    "    \n",
    "    Parameters:\n",
    "    - vals: numpy array of signal values in the main window\n",
    "    - wide_values: optional array for weighted sum calculation\n",
    "    - weights: optional weights for wide_values\n",
    "    - num_bins: number of bins for spatial features\n",
    "    - bin_stats: list of statistics to calculate per bin. \n",
    "                 Options: 'mean', 'min', 'max', 'std', 'median', 'sum'\n",
    "    \n",
    "    Returns:\n",
    "    - list of feature values\n",
    "    - list of feature names\n",
    "    \"\"\"\n",
    "    features = []\n",
    "    feature_names = []\n",
    "    \n",
    "    # Basic statistics (global)\n",
    "    if vals.size == 0:\n",
    "        basic_features = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
    "        basic_names = [\"avg\", \"min\", \"max\", \"std\", \"med\", \"sum\"]\n",
    "    else:\n",
    "        basic_features = [\n",
    "            np.mean(vals),\n",
    "            np.min(vals),\n",
    "            np.max(vals),\n",
    "            np.std(vals),\n",
    "            np.median(vals),\n",
    "            np.sum(vals)\n",
    "        ]\n",
    "        basic_names = [\"avg\", \"min\", \"max\", \"std\", \"med\", \"sum\"]\n",
    "    \n",
    "    features.extend(basic_features)\n",
    "    feature_names.extend(basic_names)\n",
    "    \n",
    "    # Weighted sum (distance-weighted)\n",
    "    if wide_values is not None and weights is not None:\n",
    "        w_sum = np.sum(wide_values * weights)\n",
    "    else:\n",
    "        w_sum = 0.0\n",
    "    features.append(w_sum)\n",
    "    feature_names.append(\"w_sum\")\n",
    "    \n",
    "    # Binned features with multiple statistics\n",
    "    bin_size = len(vals) // num_bins if len(vals) > 0 else 0\n",
    "    \n",
    "    # Mapping of stat names to functions\n",
    "    stat_functions = {\n",
    "        'mean': np.mean,\n",
    "        'min': np.min,\n",
    "        'max': np.max,\n",
    "        'std': np.std,\n",
    "        'median': np.median,\n",
    "        'sum': np.sum\n",
    "    }\n",
    "    \n",
    "    if bin_size > 0 and len(vals) >= num_bins:\n",
    "        for b in range(num_bins):\n",
    "            bin_start = b * bin_size\n",
    "            bin_end = (b + 1) * bin_size if b < num_bins - 1 else len(vals)\n",
    "            bin_vals = vals[bin_start:bin_end]\n",
    "            \n",
    "            if len(bin_vals) > 0:\n",
    "                for stat_name in bin_stats:\n",
    "                    stat_func = stat_functions[stat_name]\n",
    "                    features.append(stat_func(bin_vals))\n",
    "                    feature_names.append(f\"bin{b}_{stat_name}\")\n",
    "            else:\n",
    "                # Empty bin\n",
    "                for stat_name in bin_stats:\n",
    "                    features.append(0.0)\n",
    "                    feature_names.append(f\"bin{b}_{stat_name}\")\n",
    "    else:\n",
    "        # Not enough data for bins\n",
    "        for b in range(num_bins):\n",
    "            for stat_name in bin_stats:\n",
    "                features.append(0.0)\n",
    "                feature_names.append(f\"bin{b}_{stat_name}\")\n",
    "    \n",
    "    return features, feature_names\n",
    "\n",
    "\n",
    "def extract_all_features(gene_info_df, cell_line, window=10000, bw_paths=None, \n",
    "                         num_bins=5, bin_stats=['mean', 'max', 'std']):\n",
    "    \"\"\"\n",
    "    Extract summary statistics from bigWig files for each gene in `gene_info_df` and\n",
    "    each mark defined in `bw_paths` (or `data_paths_bw[cell_line]` by default).\n",
    "\n",
    "    Parameters:\n",
    "    - gene_info_df: pd.DataFrame with at least columns ['chr', 'tss'] and optionally 'gene_name'.\n",
    "    - cell_line: one of the keys in data_paths_bw (e.g. 'X1').\n",
    "    - window: int, number of bases upstream/downstream of TSS to include (default 10000 bases).\n",
    "    - bw_paths: optional dict of mark->path. If None, uses data_paths_bw[cell_line].\n",
    "    - num_bins: int, number of bins for spatial features (default 10).\n",
    "    - bin_stats: list of statistics to calculate per bin (default ['mean', 'max', 'std']).\n",
    "                 Options: 'mean', 'min', 'max', 'std', 'median', 'sum'\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: index matches gene order (uses 'gene_name' if present), \n",
    "                    columns like '<MARK>_avg', '<MARK>_bin0_mean', '<MARK>_bin0_max', etc.\n",
    "    \"\"\"\n",
    "    if bw_paths is None:\n",
    "        bw_paths = data_paths_bw.get(cell_line, {})\n",
    "\n",
    "    # Prepare index / gene identifiers\n",
    "    if 'gene_name' in gene_info_df.columns:\n",
    "        gene_names = gene_info_df['gene_name'].astype(str).tolist()\n",
    "    else:\n",
    "        gene_names = gene_info_df.index.astype(str).tolist()\n",
    "\n",
    "    feature_frames = []\n",
    "\n",
    "    for mark, path in bw_paths.items():\n",
    "        print(f\"Extracting features for {mark} from {path}...\")\n",
    "\n",
    "        # We'll determine column names from the first successful feature calculation\n",
    "        cols = None\n",
    "        rows = []\n",
    "\n",
    "        # Check file exists\n",
    "        try:\n",
    "            bw = pybigtools.open(path)\n",
    "            if bw is None:\n",
    "                raise FileNotFoundError(f\"Could not open bigWig: {path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Warning: cannot open {path}: {e}. Filling zeros for this mark.\")\n",
    "            # Get feature names from a dummy calculation\n",
    "            dummy_features, dummy_names = calculate_features(\n",
    "                np.array([]), num_bins=num_bins, bin_stats=bin_stats\n",
    "            )\n",
    "            cols = [mark + \"_\" + name for name in dummy_names]\n",
    "            rows = [[0.0] * len(cols) for _ in range(len(gene_names))]\n",
    "            feature_frames.append(pd.DataFrame(rows, columns=cols, index=gene_names))\n",
    "            continue\n",
    "\n",
    "        # Iterate genes in order and extract statistics\n",
    "        for i, row in enumerate(gene_info_df.itertuples(index=False)):\n",
    "            chrom = str(getattr(row, 'chr')) if 'chr' in gene_info_df.columns else str(row[0])\n",
    "            tss_start = int(getattr(row, 'TSS_start'))\n",
    "            tss_end = int(getattr(row, 'TSS_end'))\n",
    "            start = tss_start - window\n",
    "            end = tss_end + window\n",
    "            \n",
    "            # Wide window for weighted features\n",
    "            wide_window = 10000\n",
    "            wide_bins = (wide_window * 2) // 10\n",
    "            EPS_DIST = 10\n",
    "\n",
    "            try:\n",
    "                # Extract main window values\n",
    "                values = bw.values(chrom, start, end, oob=0.0)\n",
    "                \n",
    "                # Extract wide window values for weighted sum\n",
    "                wide_values = bw.values(\n",
    "                    chrom, \n",
    "                    tss_start - wide_window, \n",
    "                    tss_start + wide_window, \n",
    "                    oob=0.0, \n",
    "                    bins=wide_bins\n",
    "                )\n",
    "                \n",
    "                # Calculate distance weights\n",
    "                relative_pos = np.arange(wide_window * 2)[::((wide_window * 2) // wide_bins)]\n",
    "                distance_to_tss = np.abs(relative_pos - wide_window)\n",
    "                weights = 1.0 / (distance_to_tss + EPS_DIST)\n",
    "                close_mask = distance_to_tss <= 1000\n",
    "                weights[close_mask] = 0.0\n",
    "\n",
    "                # Handle None values from bigWig\n",
    "                if values is None:\n",
    "                    vals = np.array([], dtype=float)\n",
    "                else:\n",
    "                    vals = values\n",
    "                \n",
    "                # Calculate all features\n",
    "                gene_features, feature_names = calculate_features(\n",
    "                    vals, \n",
    "                    wide_values=wide_values, \n",
    "                    weights=weights,\n",
    "                    num_bins=num_bins,\n",
    "                    bin_stats=bin_stats\n",
    "                )\n",
    "                \n",
    "                # Set column names on first successful extraction\n",
    "                if cols is None:\n",
    "                    cols = [mark + \"_\" + name for name in feature_names]\n",
    "                \n",
    "                rows.append(gene_features)\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Keep alignment with genes even when extraction fails\n",
    "                print(f\"  Warning: failed for {chrom}:{start}-{end} ({e}). Using zeros.\")\n",
    "                if cols is None:\n",
    "                    # Get column names from dummy calculation\n",
    "                    dummy_features, dummy_names = calculate_features(\n",
    "                        np.array([]), num_bins=num_bins, bin_stats=bin_stats\n",
    "                    )\n",
    "                    cols = [mark + \"_\" + name for name in dummy_names]\n",
    "                rows.append([0.0] * len(cols))\n",
    "\n",
    "        # Close bigWig handle\n",
    "        try:\n",
    "            bw.close()\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # Create dataframe for this mark\n",
    "        mark_df = pd.DataFrame(rows, columns=cols, index=gene_names)\n",
    "        feature_frames.append(mark_df)\n",
    "\n",
    "    # Concatenate horizontally and ensure index is gene names\n",
    "    if feature_frames:\n",
    "        all_features_df = pd.concat(feature_frames, axis=1)\n",
    "        all_features_df.index.name = 'gene_name'\n",
    "    else:\n",
    "        all_features_df = pd.DataFrame(index=gene_names)\n",
    "\n",
    "    return all_features_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e74bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Approach 1: Use bw files, compute average, min, max, std over a region that spans the transcription start site (TSS) +/- 10kb\n",
    "\n",
    "# Idea: Function that given cell line and type (train/val) extracts features and targets\n",
    "def get_dataset(cell_line, set_type, window=10000, num_bins=10):\n",
    "\n",
    "    gene_info_df = pd.read_csv(gene_paths[cell_line][set_type]['info'], sep='\\t')\n",
    "    gene_target_df = pd.read_csv(gene_paths[cell_line][set_type]['target'], sep='\\t')\n",
    "    \n",
    "    # Pass window (in bases) through to feature extraction so you can vary it between runs\n",
    "    features = extract_all_features(gene_info_df, cell_line, window=window, num_bins=num_bins)\n",
    "    # targets = gene_target_df['gex'].values\n",
    "    targets = np.log2(gene_target_df['gex'].values + 0.001) # log scaling targets\n",
    "    # targets = np.arcsinh(gene_target_df['gex'].values) # arcsinh scaling targets\n",
    "\n",
    "    # features = (features+0.001).apply(np.log2) # log scale features\n",
    "    \n",
    "    return features, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df220d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faster: parallel loader\n",
    "\n",
    "def get_data_dict_joblib(n_jobs=None, window=10000, num_bins=5):\n",
    "    \"\"\"\n",
    "    Notebook-friendly parallel loader using joblib (loky backend).\n",
    "    - n_jobs: number of parallel jobs (default = min(CPU, num_tasks))\n",
    "    - window: forwarded to get_dataset/get_dataset_cached\n",
    "    \"\"\"\n",
    "    cell_lines = ['X1', 'X2']\n",
    "    splits = ['train', 'validation']\n",
    "    tasks = [(c, s) for c in cell_lines for s in splits]\n",
    "    num_tasks = len(tasks)\n",
    "    if n_jobs is None:\n",
    "        n_jobs = min(multiprocessing.cpu_count(), num_tasks)\n",
    "    else:\n",
    "        n_jobs = max(1, min(n_jobs, num_tasks))\n",
    "\n",
    "    # joblib will use loky by default in recent versions; force loky backend for robustness in notebooks\n",
    "    results = Parallel(n_jobs=n_jobs, backend='loky')(\n",
    "        delayed(get_dataset)(c, s, window, num_bins) for (c, s) in tasks\n",
    "    )\n",
    "\n",
    "    data_dict = {c: {} for c in cell_lines}\n",
    "    for (c, s), res in zip(tasks, results):\n",
    "        data_dict[c][s] = res\n",
    "\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2797e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_large_data_dict(windows=[10000], num_bins_list=[5]):\n",
    "    data_dict = {}\n",
    "    for window in windows:\n",
    "        for num_bins in num_bins_list:\n",
    "            key = f\"win_{window}__bins_{num_bins}\"\n",
    "            print(f\"Loading data for {key}...\")\n",
    "            data_dict[key] = get_data_dict_joblib(window=window, num_bins=num_bins)\n",
    "    return data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9690aa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_dict):\n",
    "    \"\"\"\n",
    "    Evaluate model performance across different training and validation combinations.\n",
    "    \n",
    "    Pipeline:\n",
    "    1) Get all data (X1 train/val, X2 train/val)\n",
    "    2) Train on: a) X1 only, b) X2 only, c) X1+X2 combined\n",
    "    3) Validate each trained model on both X1 and X2 validation sets\n",
    "    \n",
    "    Parameters:\n",
    "    - model: sklearn-compatible model instance\n",
    "    - data_dict: nested dict with structure {cell_line: {'train': (X, y), 'validation': (X, y)}}\n",
    "    \n",
    "    Returns:\n",
    "    - results: dict with keys (train_cell_line, val_cell_line) and values containing metrics\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Define training scenarios\n",
    "    training_scenarios = [\n",
    "        ('X1', ['X1']),\n",
    "        ('X2', ['X2']),\n",
    "        ('X1+X2', ['X1', 'X2'])\n",
    "    ]\n",
    "    \n",
    "    for scenario_name, train_cell_lines in training_scenarios:\n",
    "        # Prepare training data\n",
    "        X_train, y_train = _combine_datasets(data_dict, train_cell_lines, split='train')\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = StandardScaler()\n",
    "        scaler.set_output(transform=\"pandas\")\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        \n",
    "        # Feature selection (currently using all features)\n",
    "        num_features = X_train_scaled.shape[1]\n",
    "        selector = SelectKBest(f_regression, k=num_features)\n",
    "        X_train_reduced = selector.fit_transform(X_train_scaled, y_train)\n",
    "        \n",
    "        # Train model\n",
    "        model.fit(X_train_reduced, y_train)\n",
    "        \n",
    "        # Evaluate on both validation sets\n",
    "        for val_cell_line in ['X1', 'X2']:\n",
    "            metrics = _evaluate_on_validation(\n",
    "                model, \n",
    "                data_dict[val_cell_line]['validation'],\n",
    "                scaler,\n",
    "                selector\n",
    "            )\n",
    "            \n",
    "            results[(scenario_name, val_cell_line)] = metrics\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def _combine_datasets(data_dict, cell_lines, split='train'):\n",
    "    \"\"\"\n",
    "    Combine data from multiple cell lines.\n",
    "    \n",
    "    Parameters:\n",
    "    - data_dict: nested dict with cell line data\n",
    "    - cell_lines: list of cell line names to combine\n",
    "    - split: 'train' or 'validation'\n",
    "    \n",
    "    Returns:\n",
    "    - X: combined feature DataFrame\n",
    "    - y: combined target array\n",
    "    \"\"\"\n",
    "    if len(cell_lines) == 1:\n",
    "        return data_dict[cell_lines[0]][split]\n",
    "    \n",
    "    X_combined = pd.concat([data_dict[cell][split][0] for cell in cell_lines])\n",
    "    y_combined = np.concatenate([data_dict[cell][split][1] for cell in cell_lines])\n",
    "    \n",
    "    return X_combined, y_combined\n",
    "\n",
    "\n",
    "def _evaluate_on_validation(model, validation_data, scaler, selector):\n",
    "    \"\"\"\n",
    "    Evaluate model on a validation set.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: trained model\n",
    "    - validation_data: tuple of (X_val, y_val)\n",
    "    - scaler: fitted StandardScaler\n",
    "    - selector: fitted feature selector\n",
    "    \n",
    "    Returns:\n",
    "    - dict with MSE, R^2, and Spearman's rho\n",
    "    \"\"\"\n",
    "    X_val, y_val = validation_data\n",
    "    \n",
    "    # Transform validation data using fitted scaler and selector\n",
    "    X_val_scaled = scaler.transform(X_val)\n",
    "    X_val_reduced = selector.transform(X_val_scaled)\n",
    "    \n",
    "    # Predict\n",
    "    y_val_pred = model.predict(X_val_reduced)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(y_val, y_val_pred)\n",
    "    r2 = r2_score(y_val, y_val_pred)\n",
    "    spearmanr_corr, _ = spearmanr(y_val, y_val_pred)\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'R^2': r2,\n",
    "        \"Spearman's rho\": spearmanr_corr\n",
    "    }\n",
    "\n",
    "\n",
    "def print_evaluation_results(results):\n",
    "    \"\"\"\n",
    "    Pretty print evaluation results.\n",
    "    \n",
    "    Parameters:\n",
    "    - results: dict returned by evaluate_model\n",
    "    \"\"\"\n",
    "    print(\"\\nModel Evaluation Results\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    for (train_scenario, val_cell_line), metrics in sorted(results.items()):\n",
    "        print(f\"\\nTrained on: {train_scenario} | Validated on: {val_cell_line}\")\n",
    "        print(\"-\" * 70)\n",
    "        for metric_name, value in metrics.items():\n",
    "            print(f\"  {metric_name:20s}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf0e80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_results_to_dataframe(large_data_dict, models):\n",
    "    \"\"\"\n",
    "    Evaluate multiple models on multiple datasets and collect results in a DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    - large_data_dict: dict of {config_key: data_dict}\n",
    "    - models: dict of {model_name: model_instance}\n",
    "    \n",
    "    Returns:\n",
    "    - pd.DataFrame with columns: Config, Model, Train_On, Val_On, MSE, R^2, Spearman's rho\n",
    "    \"\"\"\n",
    "    all_results = []\n",
    "    \n",
    "    for config_key, data_dict in large_data_dict.items():\n",
    "        print(f\"\\nEvaluating models with config: {config_key}\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for model_name, model in models.items():\n",
    "            print(f\"  Model: {model_name}\")\n",
    "            \n",
    "            # Evaluate model\n",
    "            results = evaluate_model(model, data_dict)\n",
    "            \n",
    "            # Convert results to rows\n",
    "            for (train_scenario, val_cell_line), metrics in results.items():\n",
    "                row = {\n",
    "                    'Config': config_key,\n",
    "                    'Model': model_name,\n",
    "                    'Train_On': train_scenario,\n",
    "                    'Val_On': val_cell_line,\n",
    "                    'MSE': metrics['MSE'],\n",
    "                    'R^2': metrics['R^2'],\n",
    "                    \"Spearman's rho\": metrics[\"Spearman's rho\"]\n",
    "                }\n",
    "                all_results.append(row)\n",
    "\n",
    "                spearman_corr = metrics[\"Spearman's rho\"]\n",
    "                # Print progress\n",
    "                print(f\"    {train_scenario} → {val_cell_line}: \"\n",
    "                      f\"R²={metrics['R^2']:.4f}, \"\n",
    "                      f\"Spearman={spearman_corr:.4f}\")\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf4b738",
   "metadata": {},
   "outputs": [],
   "source": [
    "window_sizes = [100, 300, 500, 1000]\n",
    "bins = [5, 10]\n",
    "\n",
    "large_data_dict = build_large_data_dict(windows=window_sizes, num_bins_list=bins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac754b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso Regression': Lasso(alpha=0.1),\n",
    "    'HistGradientBoosting': HistGradientBoostingRegressor(random_state=42),\n",
    "    'XGBoost': xgb.XGBRegressor(n_estimators=100, n_jobs=-1, colsample_bytree=0.5, \n",
    "                                max_depth=2, random_state=42),\n",
    "    'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42, n_jobs=-1, \n",
    "                                  colsample_bytree=0.8),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1, \n",
    "                                          max_depth=10, max_features=0.1),\n",
    "}\n",
    "\n",
    "# Collect all results\n",
    "results_df = collect_results_to_dataframe(large_data_dict, models)\n",
    "\n",
    "print(\"Results DataFrame:\")\n",
    "print(results_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cee2f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train best model on all data and generate predictions for X3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
